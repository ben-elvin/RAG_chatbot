{"cells":[{"cell_type":"markdown","metadata":{"id":"W4lmzdFaqmgr"},"source":["# Streamlit\n","Streamlit lets you transform your data scripts into interactive dashboards and prototypes in minutes, without needing front-end coding knowledge. This means you can easily share insights with colleagues, showcase your data science work, or even build simple machine learning tools, all within the familiar Python environment."]},{"cell_type":"markdown","metadata":{"id":"g2MN3UoMhEdj"},"source":["---\n","## 1.\u0026nbsp; Streamlit demo ğŸš€\n","We first need to install [streamlit](https://streamlit.io/) - as always, locally this is a one time thing, whereas on colab we need to do it each session.\n","\n","We're also installing [localtunnel](https://theboroer.github.io/localtunnel-www/) here, which allows us to run streamlit from colab. If you're working locally, you don't need to install this."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23390,"status":"ok","timestamp":1718115579500,"user":{"displayName":"Ben Elvin","userId":"09111547648527423041"},"user_tz":-120},"id":"_NGG2EFdgiPL","outputId":"aa579477-a162-45fd-b9fd-4f9e7887d68e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n","\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n","\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n","\u001b[0m\n","+ localtunnel@2.0.2\n","added 22 packages from 22 contributors and audited 22 packages in 2.723s\n","\n","3 packages are looking for funding\n","  run `npm fund` for details\n","\n","found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n","  run `npm audit fix` to fix them, or `npm audit` for details\n","\u001b[K\u001b[?25h"]}],"source":["!pip install -qqq -U streamlit\n","!npm install -qqq -U localtunnel"]},{"cell_type":"markdown","metadata":{"id":"A6IbJEYVtNFI"},"source":["Here's an example of what you can produce with streamlit. It's so easy, just a few lines of python depending on what you want, and so many options!\n","\n","- Locally you would write this script in a .py file and not a notebook (.ipynb).\n","\n","- On colab, we can create a .py file by using the magic command `%%writefile` at the top of the cell. This command writes the cell content to a file, naming it 'app.py', or whatever else you choose, in this instance. Once saved, you can see 'app.py' in Colab's storage by clicking on the left-hand side folder icon."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":367,"status":"ok","timestamp":1718115598508,"user":{"displayName":"Ben Elvin","userId":"09111547648527423041"},"user_tz":-120},"id":"BOXdinLGgriU","outputId":"ba958d64-3825-4d70-80bf-0746ca13d589"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing app.py\n"]}],"source":["%%writefile app.py\n","\n","import streamlit as st\n","\n","# Title\n","st.title(\"Streamlit Demo\")\n","\n","# Markdown\n","st.markdown(\"\"\"\n","This is a demo app showcasing a few of Streamlit's features.\n","\n","Streamlit is a powerful Python library for creating web apps. It is easy to use and has a wide range of features, including:\n","\n","* **Interactive widgets:** Streamlit makes it easy to create interactive widgets, such as sliders, dropdown menus, and radio buttons.\n","* **Charts and graphs:** Streamlit can generate a variety of charts and graphs, including line charts, bar charts, and pie charts.\n","* **Data display:** Streamlit can display data in a variety of ways, including tables, lists, and maps.\n","* **Deployment:** Streamlit apps can be deployed to Heroku with a single command.\n","\"\"\")\n","\n","# Slider\n","slider_value = st.slider(\"Select a number:\", 0, 100)\n","st.write(f\"You selected: {slider_value}\")\n","\n","# Dropdown menu\n","dropdown_value = st.selectbox(\"Choose a color:\", [\"red\", \"green\", \"blue\"])\n","st.write(f\"You chose: {dropdown_value}\")\n","\n","# Radio buttons\n","radio_button_value = st.radio(\"Select a language:\", [\"English\", \"Spanish\", \"French\"])\n","st.write(f\"You selected: {radio_button_value}\")\n","\n","# Text area\n","text = st.text_area(\"Enter some text:\")\n","if text:\n","    st.write(f\"You entered: {text}\")\n","\n","# Button\n","if st.button(\"Click me!\"):\n","    st.write(\"You clicked the button!\")\n","\n","# Chart\n","data = {\"x\": [1, 2, 3, 4, 5], \"y\": [6, 7, 2, 4, 5]}\n","st.line_chart(data)\n","\n","# Map\n","map_data = [\n","    {\"name\": \"New York\", \"lat\": 40.7128, \"lon\": -74.0060},\n","    {\"name\": \"Los Angeles\", \"lat\": 34.0522, \"lon\": -118.2437},\n","    {\"name\": \"Chicago\", \"lat\": 41.8783, \"lon\": -87.6233},\n","]\n","st.map(map_data)"]},{"cell_type":"markdown","metadata":{"id":"37HjErWnt9-I"},"source":["To run streamlit apps **locally**. Open the command line and navigate to the folder where you've stored the .py file. Then, use the command `streamlit run app.py`. If you used a different name for the .py file, change `app.py` for the name you used\n","\n","On **colab**, as we have to run streamlit through a tunnel. This is a little annoying for debugging, as every time you encounter a bug, you have to stop and reopen the tunnel. However, if you have a slower computer, or you simply wish to use Google's power so that your resources are free to do other things, it's very useful.\n","\n","When opening the tunnel it will ask you for a password. The password is your IP address, which will be printed out as the first element of the output from the code below."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":167697,"status":"ok","timestamp":1718115855396,"user":{"displayName":"Ben Elvin","userId":"09111547648527423041"},"user_tz":-120},"id":"aRcEI6cjgtrW","outputId":"c90d5be9-b1db-4d83-e16b-bf2eff6ee853"},"outputs":[{"name":"stdout","output_type":"stream","text":["34.48.39.61\n","\u001b[K\u001b[?25hnpx: installed 22 in 2.102s\n","your url is: https://many-books-follow.loca.lt\n"]}],"source":["!streamlit run app.py \u0026\u003e/content/logs.txt \u0026 npx localtunnel --port 8501 \u0026 curl ipv4.icanhazip.com"]},{"cell_type":"markdown","metadata":{"id":"lUCdhQSihG5l"},"source":["## 2.\u0026nbsp; RAG chatbot in streamlit â­ï¸\n","We'll start by installing the same libraries and downloading the same files as in the previous notebooks."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110278,"status":"ok","timestamp":1718115968018,"user":{"displayName":"Ben Elvin","userId":"09111547648527423041"},"user_tz":-120},"id":"2rOEYKWwhI-d","outputId":"c37cf365-a26d-4525-bb45-13acaab29ea6"},"outputs":[{"name":"stdout","output_type":"stream","text":["     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 314.7/314.7 kB 5.0 MB/s eta 0:00:00\n","     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 227.1/227.1 kB 13.9 MB/s eta 0:00:00\n","     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 124.9/124.9 kB 14.7 MB/s eta 0:00:00\n","     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 53.0/53.0 kB 6.8 MB/s eta 0:00:00\n","     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 142.7/142.7 kB 17.8 MB/s eta 0:00:00\n","     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 21.3/21.3 MB 66.1 MB/s eta 0:00:00\n","     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 974.0/974.0 kB 13.7 MB/s eta 0:00:00\n","     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2.2/2.2 MB 29.4 MB/s eta 0:00:00\n","     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 49.2/49.2 kB 6.5 MB/s eta 0:00:00\n","     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 27.0/27.0 MB 38.5 MB/s eta 0:00:00\n","Processing file 1h_lk4wTr12FAEaCS3eIJ4xsdcmnuIGmt index.faiss\n","Processing file 1O0Jz2Lx5cZdpQM7S5uw6Kx9_OLm5DuSQ index.pkl\n"]},{"name":"stderr","output_type":"stream","text":["Retrieving folder contents\n","Retrieving folder contents completed\n","Building directory structure\n","Building directory structure completed\n","Downloading...\n","From: https://drive.google.com/uc?id=1h_lk4wTr12FAEaCS3eIJ4xsdcmnuIGmt\n","To: /content/faiss_index/index.faiss\n","\r  0%|          | 0.00/421k [00:00\u003c?, ?B/s]\r100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 421k/421k [00:00\u003c00:00, 31.5MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1O0Jz2Lx5cZdpQM7S5uw6Kx9_OLm5DuSQ\n","To: /content/faiss_index/index.pkl\n","\r  0%|          | 0.00/216k [00:00\u003c?, ?B/s]\r100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 216k/216k [00:00\u003c00:00, 7.76MB/s]\n","Download completed\n"]}],"source":["%%bash\n","pip install -qqq -U langchain-huggingface\n","pip install -qqq -U langchain\n","pip install -qqq -U langchain-community\n","pip install -qqq -U faiss-cpu\n","\n","# download saved vector database for Alice's Adventures in Wonderland\n","gdown --folder 1A8A9lhcUXUKRrtCe7rckMlQtgmfLZRQH"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":1373,"status":"ok","timestamp":1718115978490,"user":{"displayName":"Ben Elvin","userId":"09111547648527423041"},"user_tz":-120},"id":"N7cP3jlfg5jZ"},"outputs":[],"source":["import os\n","from google.colab import userdata # we stored our access token as a colab secret\n","\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')"]},{"cell_type":"markdown","metadata":{"id":"ogWDZ5fbv7BN"},"source":["Now, let's proceed by creating the .py file for our rag chatbot.\n","\n","We sourced the foundational code for our Streamlit basic chatbot from the [Streamlit documentation](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps).\n","\n","In addition, we implemented [cache_resource](https://docs.streamlit.io/library/api-reference/performance/st.cache_resource) for both memory and LLM. Given that Streamlit reruns the entire script with each message input, relying solely on memory would result in data overwriting and a loss of conversational continuity. The inclusion of cache resource prevents Streamlit from creating a new memory instance on each run. This was also added to the LLM, enhancing speed and preventing its reload in every iteration."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1718115969508,"user":{"displayName":"Ben Elvin","userId":"09111547648527423041"},"user_tz":-120},"id":"kauMaNJ1hXNP","outputId":"00849d41-9cd9-4b8a-cf59-d1b3808dcbfe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing rag_app.py\n"]}],"source":["%%writefile rag_app.py\n","\n","from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain_core.prompts import PromptTemplate\n","from langchain.memory import ConversationBufferMemory\n","from langchain.chains import ConversationalRetrievalChain\n","import streamlit as st\n","\n","# llm\n","hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","llm = HuggingFaceEndpoint(repo_id=hf_model)\n","\n","# embeddings\n","embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n","embeddings_folder = \"/content/\"\n","\n","embeddings = HuggingFaceEmbeddings(model_name=embedding_model,\n","                                   cache_folder=embeddings_folder)\n","\n","# load Vector Database\n","# allow_dangerous_deserialization is needed. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine\n","vector_db = FAISS.load_local(\"/content/faiss_index\", embeddings, allow_dangerous_deserialization=True)\n","\n","# retriever\n","retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n","\n","# memory\n","@st.cache_resource\n","def init_memory(_llm):\n","    return ConversationBufferMemory(\n","        llm=llm,\n","        output_key='answer',\n","        memory_key='chat_history',\n","        return_messages=True)\n","memory = init_memory(llm)\n","\n","# prompt\n","template = \"\"\"You are a nice chatbot having a conversation with a human. Answer the question based only on the following context and previous conversation. Keep your answers short and succinct.\n","\n","Previous conversation:\n","{chat_history}\n","\n","Context to answer question:\n","{context}\n","\n","New human question: {question}\n","Response:\"\"\"\n","\n","prompt = PromptTemplate(template=template,\n","                        input_variables=[\"context\", \"question\"])\n","\n","# chain\n","chain = ConversationalRetrievalChain.from_llm(llm,\n","                                              retriever=retriever,\n","                                              memory=memory,\n","                                              return_source_documents=True,\n","                                              combine_docs_chain_kwargs={\"prompt\": prompt})\n","\n","\n","##### streamlit #####\n","\n","st.title(\"Chatier \u0026 chatier: conversations in Wonderland\")\n","\n","# Initialise chat history\n","# Chat history saves the previous messages to be displayed\n","if \"messages\" not in st.session_state:\n","    st.session_state.messages = []\n","\n","# Display chat messages from history on app rerun\n","for message in st.session_state.messages:\n","    with st.chat_message(message[\"role\"]):\n","        st.markdown(message[\"content\"])\n","\n","# React to user input\n","if prompt := st.chat_input(\"Curious minds wanted!\"):\n","\n","    # Display user message in chat message container\n","    st.chat_message(\"user\").markdown(prompt)\n","\n","    # Add user message to chat history\n","    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n","\n","    # Begin spinner before answering question so it's there for the duration\n","    with st.spinner(\"Going down the rabbithole for answers...\"):\n","\n","        # send question to chain to get answer\n","        answer = chain(prompt)\n","\n","        # extract answer from dictionary returned by chain\n","        response = answer[\"answer\"]\n","\n","        # Display chatbot response in chat message container\n","        with st.chat_message(\"assistant\"):\n","            st.markdown(answer[\"answer\"])\n","\n","        # Add assistant response to chat history\n","        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"]},{"cell_type":"markdown","metadata":{"id":"YqnlHR1Uv-88"},"source":["Now, let's load a tunnel and see what we've made"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"YQD-kDYbhZUV"},"outputs":[{"name":"stdout","output_type":"stream","text":["34.48.39.61\n","\u001b[K\u001b[?25hnpx: installed 22 in 3.318s\n","your url is: https://calm-mirrors-relate.loca.lt\n"]}],"source":["!streamlit run rag_app.py \u0026\u003e/content/logs.txt \u0026 npx localtunnel --port 8501 \u0026 curl ipv4.icanhazip.com"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lAflyA5fibym"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPQ1mdPqLbCNhVgj9W6Rqn5","name":"","provenance":[{"file_id":"1OAzQ-gs50B1Y6ym0LHTd9mEhAJKYG-R7","timestamp":1717425080144}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}