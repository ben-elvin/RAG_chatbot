{"cells":[{"cell_type":"markdown","source":["# Large Language Models\n","\n","In the ever-evolving landscape of artificial intelligence, large language models (LLMs) have emerged as transformative tools, reshaping the way we engage with and analyse language. These sophisticated models, honed on massive repositories of text data, possess the remarkable ability to comprehend, generate, and translate human language with unprecedented accuracy and fluency. Among the prominent LLM frameworks, LangChain stands out for its efficiency and flexibility."],"metadata":{"id":"6P1KRoMREcQO"}},{"cell_type":"markdown","source":["---\n","## 1.&nbsp; Installations and Settings ðŸ› ï¸\n","\n","LangChain is a framework that simplifies the development of applications powered by large language models (LLMs). Here we install their HuggingFace package as we'll be using open source models from HuggingFace."],"metadata":{"id":"2R5vGxfp_d6w"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lIYdn1woOS1n","executionInfo":{"status":"ok","timestamp":1718096020286,"user_tz":-120,"elapsed":93052,"user":{"displayName":"Ben Elvin","userId":"09111547648527423041"}},"outputId":"edc8045e-83f2-42e7-b0ff-b1bdb4392281"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/314.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.6/314.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m662.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -qqq -U langchain-huggingface"]},{"cell_type":"markdown","source":["To use the LLMs, you'll need to create a HuggingFace access token for this project.\n","1. Sign up for an account at [HuggingFace](https://huggingface.co/)\n","2. Go in to your account and click `edit profile`\n","3. Go to `Access Tokens` and create a `New Token`\n","4. The `Type` of the new token should be set to `Read`\n","\n","We've then saved ours as a Colab secret - this way we can use it in multiple notebooks without having to type it or reveal it."],"metadata":{"id":"VbaYjoSFEl0p"}},{"cell_type":"code","source":["import os\n","from google.colab import userdata\n","\n","# Set the token as an environ variable\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')"],"metadata":{"id":"DQiMTwbbfMaJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## 2.&nbsp; Setting up your LLM ðŸ§ \n","\n","A HuggingFace EndPoint is a service that lets you deploy machine learning models, specifically those from the HuggingFace Hub, for use in real-world applications. It basically provides the infrastructure and tools to turn your models into usable APIs. You can set up your own EndPoint and you pay for the compute resources used by the minute. However, HuggingFace generously lets us test smaller LLMs using Endpoints it's already set up for free!\n","\n","There's a limit on the size of model you can use for free. Free tier limitations for model size aren't publicly disclosed, but models exceeding 10GB are likely inaccessible.\n","\n","And, on the free tier HuggingFace prioritises fair use and might throttle heavy users. Here's what they say on their [FAQ page](https://huggingface.co/docs/api-inference/faq):\n","\n","> Rate limits:\n","The free Inference API may be rate limited for heavy use cases. We try to balance the loads evenly between all our available resources, and favoring steady flows of requests. If your account suddenly sends 10k requests then youâ€™re likely to receive 503 errors saying models are loading. In order to prevent that, you should instead try to start running queries smoothly from 0 to 10k over the course of a few minutes."],"metadata":{"id":"iF2UuXVf_mrp"}},{"cell_type":"code","source":["from langchain_huggingface import HuggingFaceEndpoint\n","\n","# This info's at the top of each HuggingFace model page\n","hf_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","\n","llm = HuggingFaceEndpoint(\n","    repo_id = hf_model,\n","    # max_new_tokens=512,\n","    temperature=0.01,\n","    top_p=0.95,\n","    repetition_penalty=1.03,\n","    # huggingfacehub_api_token = \"your_hf_token\" # Instead of passing your HuggingFace token to os, you could include it here\n",")"],"metadata":{"id":"mUqPFbDFfFkK","executionInfo":{"status":"ok","timestamp":1718096023095,"user_tz":-120,"elapsed":1679,"user":{"displayName":"Ben Elvin","userId":"09111547648527423041"}},"outputId":"a94601e3-b232-4889-a305-ec5ff357b415","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"markdown","source":["Here's a brief overview of some of the parameters:\n","* **repo_id:** The path to the HuggingFace model that will be used for generating text.\n","* **max_new_tokens:** The maximum number of tokens that the model should generate in its response.\n","* **temperature:** A value between 0 and 1 that controls the randomness of the model's generation. A lower temperature results in more predictable, constrained output, while a higher temperature yields more creative and diverse text.\n","* **top_p:** A value between 0 and 1 that controls the diversity of the model's predictions. A higher top_p value prioritizes the most probable tokens, while a lower top_p value encourages the model to explore a wider range of possibilities.\n","* **repetition_penalty** discourages repetitive outputs. It penalizes tokens that have already been generated, making the model less likely to use them again. This helps produce more diverse and interesting text.\n","\n","There are many more parameters you can play with. Check out the [Docs](https://api.python.langchain.com/en/latest/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html).\n","\n","There are also [usage examples](https://python.langchain.com/v0.2/docs/integrations/llms/huggingface_endpoint/) on LangChain's website."],"metadata":{"id":"v7DleX6dF-dj"}},{"cell_type":"markdown","source":["---\n","## 3.&nbsp; Asking your LLM questions ðŸ¤–\n","Play around and note how small changes make a big difference."],"metadata":{"id":"T9jPmyi0BufQ"}},{"cell_type":"code","source":["answer_1 = llm.invoke(\"Which animals live at the north pole?\")\n","print(answer_1)"],"metadata":{"id":"kgCYF4jCf7B8","executionInfo":{"status":"ok","timestamp":1718096023411,"user_tz":-120,"elapsed":321,"user":{"displayName":"Ben Elvin","userId":"09111547648527423041"}},"outputId":"5187629a-1190-477d-fcf6-b331c1c6b6a7","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","The Arctic is home to a variety of animals that have adapted to the cold, harsh environment. Here are some of the most common animals found in the Arctic:\n","\n","1. Polar Bears: The polar bear is the largest land carnivore and is well-adapted to life in the Arctic. They have a thick layer of body fat and a waterproof coat of fur that helps them stay warm and buoyant in the icy waters.\n","\n","2. Arctic Fox: The Arctic fox is a small, agile predator that is well-adapted to the cold. They have thick fur that changes color with the seasons, and they are excellent hunters of small mammals and birds.\n","\n","3. Reindeer: Reindeer, also known as caribou in North America, are large, hoofed mammals that are well-adapted to the Arctic. They have a thick coat of fur and a strong, curved antler that helps them navigate through the snow.\n","\n","4. Beluga Whale: The beluga whale is a white, toothed whale that lives in the Arctic waters. They are well-adapted to the cold and can dive to depths of up to 1,000 feet.\n","\n","5. Walrus: The walrus is a large, marine mammal that is well-adapted to life in the Arctic. They have a thick layer of blubber that helps them stay warm, and they have tusks that they use for hunting and fighting.\n","\n","6. Seals: There are several species of seals that live in the Arctic, including the harp seal, hooded seal, and ringed seal. They are well-adapted to life in the water and are important prey for polar bears and other predators.\n","\n","7. Arctic Tern: The Arctic tern is a migratory bird that breeds in the Arctic and spends the winter in the Antarctic. They are known for their long migration, which can cover up to 25,000 miles each way.\n","\n","8. Snowy Owl: The snowy owl is a large, white owl that is well-adapted to life in the Arctic. They have excellent vision and are skilled hunters of small mammals and birds.\n","\n","\n"]}]},{"cell_type":"code","source":["answer_2 = llm.invoke(\"Write a poem about animals that live at the north pole.\")\n","print(answer_2)"],"metadata":{"id":"cHCYwJhkf6-z","executionInfo":{"status":"ok","timestamp":1718096136614,"user_tz":-120,"elapsed":311,"user":{"displayName":"Ben Elvin","userId":"09111547648527423041"}},"outputId":"bdf1d82e-ee8e-46c8-f2c3-c7491350a6c2","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","In the realm where the sun seldom shines,\n","A world of ice and snowy lines,\n","Lives a cast of creatures, so divine,\n","The North Pole's enchanting designs.\n","\n","Polar bears, white as the snow they roam,\n","With fur so thick, they're never cold,\n","Their eyes, like stars, in the icy dome,\n","Guard the Arctic's ancient hold.\n","\n","Seals and walruses, in the ocean deep,\n","Swimming with grace, their tales unfurled,\n","Their voices echo, a haunting peep,\n","In this frozen, icy world.\n","\n","Arctic foxes, with coats so bright,\n","Dance upon the snowy plains,\n","Their agility, a sight to ignite,\n","A spectacle of nature's reigns.\n","\n","Reindeer, strong and sure on their feet,\n","Through the snow, they pull Santa's sleigh,\n","Their antlers, a testament to meet,\n","The challenges of winter's day.\n","\n","Penguins, though not native to this land,\n","Have found a home on the ice floes,\n","Their waddle and slide, a joyous band,\n","In the North Pole's icy repose.\n","\n","Each creature, in its own special way,\n","Adapts to life in this cold domain,\n","A testament to survival's sway,\n","In the North Pole's icy reign.\n"]}]},{"cell_type":"code","source":["answer_3 = llm.invoke(\"Explain the central limit theorem like I'm 5 years old.\")\n","print(answer_3)"],"metadata":{"id":"_EU7K5Raf68d","executionInfo":{"status":"ok","timestamp":1718096023412,"user_tz":-120,"elapsed":6,"user":{"displayName":"Ben Elvin","userId":"09111547648527423041"}},"outputId":"8e0fd162-375f-42cc-b1e0-a283cb3e6f35","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Alright, let's imagine you have a big bag of candies. Each candy has a different weight, but let's say the average weight of all candies is 10 grams and the weights are spread out around that average.\n","\n","Now, if you pick 5 candies at random from the bag, the total weight of those 5 candies might not be exactly 50 grams (10 grams per candy). It could be more or less. But if you do this many times, the average total weight of the candies you picked will usually be pretty close to 50 grams.\n","\n","The central limit theorem is like a rule that says this will happen, no matter what the individual candies weigh. As long as you have a lot of candies and you're picking a large enough group at a time, the average total weight of the candies you pick will be close to the average weight of an individual candy, no matter what that weight is.\n","\n","So, even if you have candies that weigh 1 gram or 100 grams, if you pick enough of them at a time, the average total weight will still be close to the average weight of an individual candy. That's the central limit theorem!\n"]}]},{"cell_type":"markdown","source":["The answers provided by the 7B model may not seem as impressive as those from the latest OpenAI or Google models, but consider the significant size difference - they perform very well. These models may not have the most extensive knowledge base, but for our purposes, we only need them to generate coherent English. We'll then infuse them with specialised knowledge on a topic of your choice, resulting in a local, specialised model that can function offline."],"metadata":{"id":"sRyF9BdXB1rU"}},{"cell_type":"markdown","source":["---\n","## 4.&nbsp; Challenge ðŸ˜€\n","Play around with this, and other, LLMs. keep a record of your findings:\n","1. Pose different questions to the model, each subtly different from the last. Observe the resulting outputs. Smaller models tend to be highly sensitive to minor changes in language and grammar.\n","2. Experiment with the parameters, one at a time, to assess their impact on the output.\n","3. Attempt to load different models. **Remember**: you can only use models under 10GB for free. This means most 7B or 8B will work, but when you move closer to 11B or 13B models, they are unlikely to function on the free tier of EndPoints. Explore the [models page on HuggingFace](https://huggingface.co/models). You can use the left hand menu to find `Text Generation` under `Natural Language Processing`. When you find a model you like, the repo id is at the top of the model card. Use this repo id to load the endpoint.\n","\n","\n","![](https://drive.google.com/uc?export=view&id=1hm_UJRaelxR1L4WRBfPJZYQyy7OS4Bj4)\n"],"metadata":{"id":"txlIhyawB43U"}},{"cell_type":"code","source":[],"metadata":{"id":"wZOsorPVf656"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"/v2/external/notebooks/empty.ipynb","timestamp":1717408157315}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}