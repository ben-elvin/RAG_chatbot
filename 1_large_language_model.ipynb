{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTUkvg4oZi+TbSH1PmVupk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Large Language Models\n","\n","In the ever-evolving landscape of artificial intelligence, large language models (LLMs) have emerged as transformative tools, reshaping the way we engage with and analyse language. These sophisticated models, honed on massive repositories of text data, possess the remarkable ability to comprehend, generate, and translate human language with unprecedented accuracy and fluency. Among the prominent LLM architectures, LangChain stands out for its efficiency and flexibility.\n","\n","This notebook is designed to seamlessly run both locally and on Google Colab. For those who may only have a CPU, there are clear instructions on how to run the notebook without a GPU. Don't worry, simply follow the instructions for either GPU or CPU, depending on your setup.\n","\n","Please note that using only a CPU will result in noticeably slower model performance."],"metadata":{"id":"rQEWtbLY3-xd"}},{"cell_type":"markdown","source":["---\n","## 1.&nbsp; Installations and Settings üõ†Ô∏è\n","\n","On Google Colab, you have access to free GPUs, whenever they're available. Let's utilise this advantage. To configure a Colab GPU, navigate to \"Edit\" and then \"Notebook Settings\". Select \"GPU\" and then click \"Save\".\n","\n","To proceed, you'll need to install two libraries: Langchain and Llama.cpp. When operating this notebook locally, you only need to install these libraries once, and they'll remain on your computer. However, in Colab, they're not default libraries and must be installed for each session.\n","\n","**LangChain** is a framework that simplifies the development of applications powered by large language models (LLMs)\n","\n","**llama.cpp** enables us to execute quantised versions of models.\n","\n","> Quantisation of LLMs is a process that reduces the precision of the numerical values in the model, such as converting 32-bit floating-point numbers to 8-bit integers. These models are therefore smaller and faster, allowing them to run on less powerful hardware with only a small loss in precision.\n","\n","* If you're using a **CPU**, use the [standard installation](https://python.langchain.com/docs/integrations/llms/llamacpp#cpu-only-installation) of llama.cpp. Windows users might have to install [a couple of extra libraries too](https://python.langchain.com/docs/integrations/llms/llamacpp#installation-with-windows).\n","* If you have an **NVIDIA GPU**, you need to [activate cuBLAS](https://python.langchain.com/docs/integrations/llms/llamacpp#installation-with-openblas-cublas-clblast) with llama.cpp. cuBLAS is a library that speeds up operations on NVIDIA GPUs.\n"," * Some students using windows have also found [this guide](https://medium.com/@piyushbatra1999/installing-llama-cpp-python-with-nvidia-gpu-acceleration-on-windows-a-short-guide-0dfac475002d) useful.\n","* If you have a **silicon chip Apple with a GPU**, you need to [enable Metal](https://python.langchain.com/docs/integrations/llms/llamacpp#installation-with-metal)."],"metadata":{"id":"vO0LbZbd4HZ-"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mJ_oTkGwppsp","executionInfo":{"status":"ok","timestamp":1713172158043,"user_tz":-120,"elapsed":134375,"user":{"displayName":"Ben Elvin","userId":"09111547648527423041"}},"outputId":"4ef6c104-7538-479c-d8d4-5a4043f15c50"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip3 install -qqq langchain --progress-bar off\n","# As this notebook is originally on Colab, here we'll use their NVIDIA GPU and activate cuBLAS\n","!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip3 install -qqq llama-cpp-python --progress-bar off"]},{"cell_type":"markdown","source":["Before we dive into the examples, let's download the large language model (LLM) we'll be using. For these exercises, we've selected a [quantised version of Mistral AI's Mistral 7B model](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF). While this is a great choice, it's by no means the only option. We encourage you to explore and try different models to discover the unique strengths and weaknesses of each. Even models of similar size can exhibit surprisingly different capabilities.\n","\n","> Since we're working in Colab, we'll need to download the LLM for each session.\n","<br>\n","If you're working locally, you can download the model once. The model is then on your computer and doesn't need to be downloaded each time. Change the `--local-dir` to your folder of choice."],"metadata":{"id":"XIolHmA0qIpT"}},{"cell_type":"code","source":["!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DKugu_x6qDbj","executionInfo":{"status":"ok","timestamp":1713172218769,"user_tz":-120,"elapsed":60739,"user":{"displayName":"Ben Elvin","userId":"09111547648527423041"}},"outputId":"3fc71889-a794-464d-d325-68019769bcd8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n","downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf to /root/.cache/huggingface/hub/tmpsj5g8lkp\n","mistral-7b-instruct-v0.1.Q4_K_M.gguf: 100% 4.37G/4.37G [00:59<00:00, 73.1MB/s]\n","./mistral-7b-instruct-v0.1.Q4_K_M.gguf\n"]}]},{"cell_type":"markdown","source":["---\n","## 2.&nbsp; Setting up your LLM üß†\n","\n","Langchain simplifies LLM deployment with its streamlined setup process. A single line of code configures your LLM, allowing you to tailor the parameters to your specific needs.\n","\n","If you want to know more about Llama.cpp, you can [read the docs here](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/). Alternatively, here are the [LangChain docs for Llama.cpp](https://python.langchain.com/docs/integrations/llms/llamacpp).\n","\n","Here's a brief overview of some of the parameters:\n","* **model_path:** The path to the Llama model file that will be used for generating text.\n","* **max_tokens:** The maximum number of tokens that the model should generate in its response.\n","* **temperature:** A value between 0 and 1 that controls the randomness of the model's generation. A lower temperature results in more predictable, constrained output, while a higher temperature yields more creative and diverse text.\n","* **top_p:** A value between 0 and 1 that controls the diversity of the model's predictions. A higher top_p value prioritizes the most probable tokens, while a lower top_p value encourages the model to explore a wider range of possibilities.\n","* **n_gpu_layers:** The default setting of 0 will cause all layers to be executed on the CPU. Setting n_gpu_layers to 1 will cause the first layer of the model to be executed on the GPU, while the remaining layers are executed on the CPU. Setting n_gpu_layers to 2 will cause the first two layers of the model to be executed on the GPU, while the remaining layers are executed on the CPU, and so on. -1 will cause all layers to be offloaded to the GPU. In general, it is a good idea to experiment with different values of n_gpu_layers to find the best balance between performance and memory usage for your specific application."],"metadata":{"id":"D7d2-gAb5Cw7"}},{"cell_type":"code","source":["from langchain.llms import LlamaCpp\n","\n","llm = LlamaCpp(model_path = \"/content/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n","               max_tokens = 2000,\n","               temperature = 0.1,\n","               top_p = 1,\n","               n_gpu_layers = -1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-F76VzycqhLv","executionInfo":{"status":"ok","timestamp":1713172221768,"user_tz":-120,"elapsed":3011,"user":{"displayName":"Ben Elvin","userId":"09111547648527423041"}},"outputId":"b0697692-baf6-4629-c96b-8edd591d7fac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /content/mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 15\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q4_K:  193 tensors\n","llama_model_loader: - type q6_K:   33 tensors\n","llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n","llm_load_print_meta: format           = GGUF V2\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 32768\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 4\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 14336\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 32768\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 7B\n","llm_load_print_meta: model ftype      = Q4_K - Medium\n","llm_load_print_meta: model params     = 7.24 B\n","llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n","llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_tensors: ggml ctx size =    0.11 MiB\n","llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",".................................................................................................\n","llama_new_context_with_model: n_ctx      = 512\n","llama_new_context_with_model: n_batch    = 8\n","llama_new_context_with_model: n_ubatch   = 8\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n","llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n","llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n","llama_new_context_with_model:        CPU compute buffer size =     1.27 MiB\n","llama_new_context_with_model: graph nodes  = 1030\n","llama_new_context_with_model: graph splits = 1\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n","Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n","Using fallback chat format: None\n"]}]},{"cell_type":"markdown","source":["If you're using a GPU, check the output of this cell ‚òùÔ∏è\n","  * If you're using cuBLAS, you'll see `BLAS = 1` if it's installed correctly.\n","  * If you're using Metal, you'll see `NEON = 1` if it's installed correctly."],"metadata":{"id":"OxhMbwWZjyOm"}},{"cell_type":"markdown","source":["---\n","## 3.&nbsp; Asking your LLM questions ü§ñ\n","Play around and note how small changes make a big difference."],"metadata":{"id":"QWG2XPn15NgV"}},{"cell_type":"code","source":["answer_1 = llm.invoke(\"Which animals live at the north pole?\")\n","print(answer_1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aCFyGekD1YWQ","executionInfo":{"status":"ok","timestamp":1713172249096,"user_tz":-120,"elapsed":27331,"user":{"displayName":"Ben Elvin","userId":"09111547648527423041"}},"outputId":"bd1d1c92-6cc7-46fa-ecb8-4ef7fc65ae42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\n","llama_print_timings:        load time =    5907.86 ms\n","llama_print_timings:      sample time =      20.56 ms /    36 runs   (    0.57 ms per token,  1751.14 tokens per second)\n","llama_print_timings: prompt eval time =    5907.78 ms /     8 tokens (  738.47 ms per token,     1.35 tokens per second)\n","llama_print_timings:        eval time =   21417.20 ms /    36 runs   (  594.92 ms per token,     1.68 tokens per second)\n","llama_print_timings:       total time =   27475.34 ms /    44 tokens\n"]},{"output_type":"stream","name":"stdout","text":["\n","Polar bears, arctic foxes, walruses, seals, and narwhals are some of the animals that live at the North Pole.\n"]}]},{"cell_type":"code","source":["answer_2 = llm.invoke(\"Write a poem about animals that live at the north pole.\")\n","print(answer_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fc8mzzu51d5b","executionInfo":{"status":"ok","timestamp":1713172462513,"user_tz":-120,"elapsed":213428,"user":{"displayName":"Ben Elvin","userId":"09111547648527423041"}},"outputId":"8353aa5b-e95b-471a-ca49-238f4d1bb5f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Llama.generate: prefix-match hit\n","\n","llama_print_timings:        load time =    5907.86 ms\n","llama_print_timings:      sample time =     208.20 ms /   336 runs   (    0.62 ms per token,  1613.81 tokens per second)\n","llama_print_timings: prompt eval time =    5914.04 ms /    12 tokens (  492.84 ms per token,     2.03 tokens per second)\n","llama_print_timings:        eval time =  205891.43 ms /   335 runs   (  614.60 ms per token,     1.63 tokens per second)\n","llama_print_timings:       total time =  213350.67 ms /   347 tokens\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","At the North Pole, where the ice is thick and cold,\n","Lives a world of animals brave and bold.\n","From the mighty polar bear to the tiny snowy owl,\n","Each one plays an important role in this arctic dwelling.\n","\n","The polar bear rules the land with strength and might,\n","A creature of power, a true arctic sight.\n","With its thick fur coat and sharp claws for fighting,\n","It's a force to be reckoned with, always hunting.\n","\n","But there are others who live in this world of snow,\n","Each one playing its part, never letting go.\n","The arctic fox, with its white fur and cunning ways,\n","Is always on the hunt for its next meal today.\n","\n","The snowy owl, with its beautiful white plumage,\n","Soars high above the world, a true arctic sage.\n","With its keen eyesight, it sees all from above,\n","A guardian of the north, a creature of love.\n","\n","And let's not forget the seals, who live beneath the ice,\n","Basking in the warmth of this arctic paradise.\n","They flip and flop, playing games and having fun,\n","In this world of animals, where everyone's happy and one.\n","\n","So next time you think of the North Pole and its cold,\n","Remember all the creatures who call it home, brave and bold.\n","From the polar bear to the snowy owl,\n","Each one plays an important role in this arctic dwelling.\n"]}]},{"cell_type":"code","source":["answer_3 = llm.invoke(\"Explain the central limit theorem like I'm 5 years old.\")\n","print(answer_3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h3LAlLON1G3g","executionInfo":{"status":"ok","timestamp":1713172524498,"user_tz":-120,"elapsed":61992,"user":{"displayName":"Ben Elvin","userId":"09111547648527423041"}},"outputId":"c5238cb8-daab-49b3-ac9d-f02cb82b59cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Llama.generate: prefix-match hit\n","\n","llama_print_timings:        load time =    5907.86 ms\n","llama_print_timings:      sample time =      56.70 ms /    92 runs   (    0.62 ms per token,  1622.49 tokens per second)\n","llama_print_timings: prompt eval time =    6490.39 ms /    15 tokens (  432.69 ms per token,     2.31 tokens per second)\n","llama_print_timings:        eval time =   54930.91 ms /    91 runs   (  603.64 ms per token,     1.66 tokens per second)\n","llama_print_timings:       total time =   61809.26 ms /   106 tokens\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","The central limit theorem is a math rule that says if you take lots of numbers and add them up, it doesn't really matter where the numbers came from or how many there are, the result will still be pretty close to normal. Like if you roll a dice 100 times, the sum of all the rolls will probably be around the middle number (50), even though some rolls might be higher or lower than that.\n"]}]},{"cell_type":"markdown","source":["The answers provided by the 7B model may not seem as impressive as those from the latest OpenAI or Google models, but consider the significant size difference - they perform very well. These models may not have the most extensive knowledge base, but for our purposes, we only need them to generate coherent English. We'll then infuse them with specialised knowledge on a topic of your choice, resulting in a local, specialised model that can function offline."],"metadata":{"id":"AnvnmmRIYQhF"}},{"cell_type":"markdown","source":["---\n","## 4.&nbsp; Challenge üòÄ\n","Play around with this, and other, LLMs. keep a record of your findings:\n","1. Pose different questions to the model, each subtly different from the last. Observe the resulting outputs. Smaller models tend to be highly sensitive to minor changes in language and grammar.\n","2. Experiment with the parameters, one at a time, to assess their impact on the output.\n","3. Attempt to load different models: Explore the [models page on HuggingFace](https://huggingface.co/models). You can use the left hand menu to find `Text Generation` under `Natural Language Processing`. Then use the filter bar for `GGUF` to find already quantised models.\n","\n","You can alter the download command accordingly. In this note book we used the command:"],"metadata":{"id":"WD4i8-3FEp04"}},{"cell_type":"code","source":["# !huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"],"metadata":{"id":"Rzkwz4cI1DgP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This downloads the version `mistral-7b-instruct-v0.1.Q4_K_M.gguf` of the model `TheBloke/Mistral-7B-Instruct-v0.1-GGUF` from huggingface. You can read about the different versions on the models `model card`.\n","\n","To adapt this just change the model and the version to your new choice.\n","\n","`!huggingface-cli download {model_name} {model_version} --local-dir . --local-dir-use-symlinks False`\n","\n","For example:"],"metadata":{"id":"fCADTg0j1xnU"}},{"cell_type":"code","source":["# !huggingface-cli download TheBloke/Llama-2-7B-Chat-GGUF llama-2-7b-chat.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"],"metadata":{"id":"R9ChvulP1Dd0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The code above would download a [quantised version of Meta's Llama 2 7B chat](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/tree/main).\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"IouC-hN42PHc"}}]}